'''
Goal of StatsModel microservice:
1. StatsModel microservice will accept the GitHub data from Flask microservice and will forecast the data for next 1 year based on past 30 days
2. It will also plot three different graph (i.e.  "Model Loss", "Prophet Generated Data", "All Issues Data") using matplot lib 
3. This graph will be stored as image in Google Cloud Storage.
4. The image URL are then returned back to Flask microservice.
'''
# Import all the required packages
from flask import Flask, jsonify, request, make_response
import os
from dateutil import *
from datetime import timedelta
import pandas as pd
from pandas import DataFrame, Series
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import time
from flask_cors import CORS
import datetime
from datetime import date
import calendar
import requests
from requests.auth import HTTPBasicAuth

# Prophet related packages
import pandas as pd
import time
import tqdm
import sklearn as sk
import github3
import json

import statsmodels
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_pacf
import pmdarima
from pmdarima.arima.utils import ndiffs
from statsmodels.tsa.statespace.sarimax import SARIMAX


# Import required storage package from Google Cloud Storage
from google.cloud import storage

# Initilize flask app
app = Flask(__name__)
# Handles CORS (cross-origin resource sharing)
CORS(app)


# Initlize Google cloud storage client
client = storage.Client()
# Add response headers to accept all types of  requests

def build_preflight_response():
    response = make_response()
    response.headers.add("Access-Control-Allow-Origin", "*")
    response.headers.add("Access-Control-Allow-Headers", "Content-Type")
    response.headers.add("Access-Control-Allow-Methods",
                         "PUT, GET, POST, DELETE, OPTIONS")
    return response

#  Modify response headers when returning to the origin

def build_actual_response(response):
    response.headers.set("Access-Control-Allow-Origin", "*")
    response.headers.set("Access-Control-Allow-Methods",
                         "PUT, GET, POST, DELETE, OPTIONS")
    return response

gitHeaders = {
    "Authorization": f"token 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",
    "Accept": "application/vnd.github.v3+json",
}

BASE_IMAGE_PATH = os.environ.get(
        'BASE_IMAGE_PATH', '/Users/anushasp/Desktop/HW5_Sonte_Parameshwar,Anusha/src/Forecasting/Prophet/')
    # DO NOT DELETE "static/images" FOLDER as it is used to store figures/images generated by matplotlib
LOCAL_IMAGE_PATH = "static/images/"
LOCAL_REPO_PATH = "static/repofiles/"

# Add your unique Bucket Name if you want to run it local
#BUCKET_NAME = os.environ.get('BUCKET_NAME', 'YOUR_BUCKET_NAME')

#bucket = client.get_bucket('YOUR_BUCKET')

def build_json_file(repo_name,repo_key, start_date, end_date):

    gitToken = os.environ.get(
        'GITHUB_TOKEN', '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')

    gh = github3.login(token='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') 

    REPO_FILENAME=  BASE_IMAGE_PATH + repo_name + '.json'

    f = open(REPO_FILENAME, 'a')
    # Issues
    issue_query = f'type:issue repo:{repo_key} created:{start_date}..{end_date}'
    for issue in gh.search_issues(issue_query):  
            label_name = []
            data = {}
            current_issue = issue.as_json()
            current_issue = json.loads(current_issue)
            data['type'] = 'issue'
            data['issue_number'] = current_issue["number"] 
            data['created_at'] = current_issue["created_at"][0:10]  
            if current_issue["closed_at"] is None:
                data['closed_at'] = current_issue["closed_at"]
            else:
                data['closed_at'] = current_issue["closed_at"][0:10]  
            for label in current_issue["labels"]:
                label_name.append(label["name"])  
            data['labels'] = label_name
            data['state'] = current_issue["state"]  
            data['author'] = current_issue["user"]["login"]  
            out = json.dumps(data)  
            f.write(out + '\n')

        
    # Pull Requests
    f = open(REPO_FILENAME, 'a')
    pr_query = f'type:pr repo:{repo_key} created:{start_date}..{end_date}'
    for issue in gh.search_issues(pr_query):
            pr_data = {}
            current_pr = issue.as_json()
            current_pr = json.loads(current_pr)
            pr_data['type'] = 'pull_request'
            pr_data['pr_number'] = current_pr["number"]
            pr_data['created_at'] = current_pr["created_at"][0:10]
            if current_pr["closed_at"] is None:
                pr_data['closed_at'] = current_pr["closed_at"]
            else:
                pr_data['closed_at'] = current_pr["closed_at"][0:10]
            pr_data['state'] = current_pr["state"]
            pr_data['author'] = current_pr["user"]["login"]
            out = json.dumps(pr_data)
            f.write(out + '\n')
    f.close()
    return REPO_FILENAME

def fetchCommitsBranchesReleasesData(repo_name,repo_key):
    if(repo_name == "OpenAI Python" or repo_name=="OpenAI Codebook"):
        repo_name = "openai"
    usersUrl = 'https://api.github.com/users/' + repo_name 
    data = requests.get(usersUrl, headers = gitHeaders)
    data = data.json()
    url = data['repos_url']
    page_no = 1
    repos_data = []
    while (True):
        response = requests.get(url, headers=gitHeaders)
        response = response.json()
        repos_data.extend(response)
        repos_fetched = len(response)
        if (repos_fetched == 30):
            page_no = page_no + 1
            url = data['repos_url'] + '?page=' + str(page_no)
        else:
            break

    repo_req = []
    k=0
    for i in range(len(repos_data)):
        if ((repos_data[i]['url'] == 'https://api.github.com/repos/' + repo_key)):
            repo_req.append(repos_data[i])
            k = k+1
    repos_information = []
    for i, repo in enumerate(repo_req):
        data = []
        data.append(repo['id'])
        data.append(repo['name'])
        data.append(repo['description'])
        data.append(repo['created_at'])
        data.append(repo['updated_at'])
        data.append(repo['owner']['login'])
        data.append(repo['license']['name'] if repo['license'] != None else None)
        data.append(repo['has_wiki'])
        data.append(repo['forks_count'])
        data.append(repo['open_issues_count'])
        data.append(repo['stargazers_count'])
        data.append(repo['watchers_count'])
        data.append(repo['url'])
        data.append(repo['commits_url'].split("{")[0])
        data.append(repo['url'] + '/branches?')
        data.append(repo['url'] + '/contributors')
        data.append(repo['releases_url'])
        data.append(repo['url'] + '/languages')
        repos_information.append(data)
    repos_df = pd.DataFrame(repos_information, columns = ['Id', 'Name', 'Description', 'Created_at', 'Closed_at','Owner', 'License', 'has_wiki','forks_count'
                                                          ,'Issues count', 'Stars count', 'Watchers count', 'Repo URL', 'Commits URL','Branches URL','Contributors URL', 'Releases URL','Languages'])
    repos_df['Releases URL']
    repos_df.to_csv(BASE_IMAGE_PATH + LOCAL_IMAGE_PATH + repo_name + 'commitsData' + '.csv', index = False)
    return repos_df

def getcommitscsv(repos_df,repo_name):
    for i in range(len(repos_df)):
        response = requests.get(repos_df.loc[i, 'Commits URL'], headers=gitHeaders)
        response.json()
    commits_information = []
    for i in range(repos_df.shape[0]):
        url = repos_df.loc[i, 'Commits URL']
        page_no = 1
        while (True):
            response = requests.get(url, headers=gitHeaders)
            response = response.json()
            for commit in response:
                commit_data = []
                commit_data.append(repos_df.loc[i, 'Id'])
                commit_data.append(commit['sha'])
                commit_data.append(commit['commit']['committer']['date'])
                commit_data.append(commit['commit']['message'])
                commits_information.append(commit_data)
            if (len(response) == 100):
                page_no = page_no + 1
            else:
                break
    commits_df = pd.DataFrame(commits_information, columns = ['Repo Id', 'Commit Id', 'Date', 'Commit Message'])
    df1 = commits_df.groupby(['Date'], as_index = False).count()
    df1['Date'] = pd.to_datetime(df1['Date'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')
    dataFrame = df1[['Date','Commit Id']]
    dataFrame.columns = ['ds', 'y']
    dataFrame['ds'] = dataFrame['ds'].dt.tz_localize('UTC').dt.tz_convert(None)
    dataFrame['ds'] = dataFrame['ds'].dt.strftime('%Y-%m-%d')
    dataFrame.to_csv(BASE_IMAGE_PATH  + repo_name + '_COMMITS_DATA' + '.csv', index = False,header=True)
    path_of_commits_csv = BASE_IMAGE_PATH  + repo_name + '_COMMITS_DATA' + '.csv'
    return path_of_commits_csv

def getbranchescsv(repos_df,repo_name):
    for i in range(len(repos_df)):
        response = requests.get(repos_df.loc[i, 'Branches URL'], headers=gitHeaders)
    branches_information = []
    for i in range(repos_df.shape[0]):
        url = repos_df.loc[i, 'Branches URL']
        page_no = 1
        while (True):
            response = requests.get(url, headers=gitHeaders)
            response = response.json()
            branch_commit_url = response
            for branch in response:
                response_branch = requests.get(branch['commit']['url'], headers=gitHeaders)
                response_branch = response_branch.json()
                branch_detail = response_branch
                branch_data = []
                branch_data.append(repos_df.loc[i, 'Id'])
                branch_data.append(branch_detail['sha'])
                branch_data.append(branch_detail['commit']['committer']['date'])
                branches_information.append(branch_data)
            if (len(response) == 100):
                page_no = page_no + 1
                url = repos_df.loc[i, 'Branches URL'] + '?page=' + str(page_no)
            else:
                break
    branches_df = pd.DataFrame(branches_information, columns = ['Repo Id', 'Branch Id', 'Date'])
    df2 = branches_df.groupby(['Date'], as_index = False).count()
    df2['Date'] = pd.to_datetime(df2['Date'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')
    dataFrame_branches = df2[['Date','Branch Id']]
    dataFrame_branches.columns = ['ds', 'y']
    dataFrame_branches['ds'] = dataFrame_branches['ds'].dt.tz_localize('UTC').dt.tz_convert(None)
    dataFrame_branches['ds'] = dataFrame_branches['ds'].dt.strftime('%Y-%m-%d')
    dataFrame_branches.to_csv(BASE_IMAGE_PATH  + repo_name + '_BRANCHES_DATA' + '.csv', index = False,header=True)
    path_of_branches_csv = BASE_IMAGE_PATH  + repo_name + '_BRANCHES_DATA' + '.csv'
    return path_of_branches_csv

def getreleasescv(repos_df,repo_name):
    for i in range(len(repos_df)):
        response = requests.get(repos_df.loc[i, 'Commits URL'], headers=gitHeaders)
        response.json()
    release_information = []
    for i in range(repos_df.shape[0]):
        url = repos_df.loc[i, 'Commits URL']
        page_no = 1
        while (True):
            response = requests.get(url, headers=gitHeaders)
            response = response.json()
            for branch in response:
                release_data = []
                release_data.append(repos_df.loc[i, 'Id'])
                release_data.append(branch['sha'])
                release_data.append(branch['commit']['committer']['date'])
                release_data.append(branch['commit']['message'])
                release_information.append(release_data)
            if (len(response) == 100):
                page_no = page_no + 1
            else:
                break
    releases_df = pd.DataFrame(release_information, columns = ['Repo Id', 'Commit Id','Date', 'Commit Message'])
    df3 = releases_df.groupby(['Date'], as_index = False).count()
    df3['Date'] = pd.to_datetime(df3['Date'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')
    dataFrameForReleases = df3[['Date','Commit Id']]
    dataFrameForReleases.columns = ['ds', 'y']
    dataFrameForReleases['ds'] = dataFrameForReleases['ds'].dt.tz_localize('UTC').dt.tz_convert(None)
    dataFrameForReleases['ds'] = dataFrameForReleases['ds'].dt.strftime('%Y-%m-%d')
    dataFrameForReleases.to_csv(BASE_IMAGE_PATH  + repo_name + '_RELEASE_DATA' + '.csv', index = False,header=True)
    path_of_releases_csv = BASE_IMAGE_PATH  + repo_name + '_RELEASE_DATA' + '.csv'
    return path_of_releases_csv

def getcontributerscsv(repos_df,repo_name):
    for i in range(len(repos_df)):
        response = requests.get(repos_df.loc[i, 'Contributors URL'],headers=gitHeaders)
        response.json()
        contributors_information = []
        for i in range(repos_df.shape[0]):
            url = repos_df.loc[i, 'Commits URL']
            page_no = 1
            while (True):
                response = requests.get(url,headers=gitHeaders)
                response = response.json()
                for contributors in response:
                    contributors_data = []
                    contributors_data.append(repos_df.loc[i, 'Id'])
                    contributors_data.append(contributors['sha'])
                    contributors_data.append(contributors['commit']['committer']['date'])
                    contributors_information.append(contributors_data)
                if (len(response) == 100):
                    page_no = page_no + 1
                    url = repos_df.loc[i, 'Contributors URL'] + '?page=' + str(page_no)
                else:
                    break
    contributors_df = pd.DataFrame(contributors_information, columns = ['Repo Id','Id', 'Date'])
    df3 = contributors_df.groupby(['Date'], as_index = False).count()
    df3['Date'] = pd.to_datetime(df3['Date'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')
    dataFrame_contributers = df3[['Date','Id']]
    dataFrame_contributers.columns = ['ds', 'y']
    dataFrame_contributers['ds'] = dataFrame_contributers['ds'].dt.tz_localize('UTC').dt.tz_convert(None)
    dataFrame_contributers['ds'] = dataFrame_contributers['ds'].dt.strftime('%Y-%m-%d')
    dataFrame_contributers.to_csv(BASE_IMAGE_PATH  + repo_name + '_CONTRIBUTERS_DATA' + '.csv', index = False,header=True)
    path_of_contributers_csv = BASE_IMAGE_PATH  + repo_name + '_CONTRIBUTERS_DATA' + '.csv'
    return path_of_contributers_csv          



def maxIssuesCreatedDate(file):
    df = pd.read_csv(file)
    df1 = df.groupby(['created_at']).count().reset_index()
    maxIssuesCreateddDate = df1.sort_values('issue_number', ascending=False).head(1).iloc[0]['created_at']
    return maxIssuesCreateddDate

def maxIssuesClosedDate(file):
    df = pd.read_csv(file)
    df1 = df.groupby(['closed_at']).count().reset_index()
    maxIssuesClosedDate = df1.sort_values('issue_number', ascending=False).head(1).iloc[0]['closed_at']
    return maxIssuesClosedDate

def convertedDate(data):
    year, month, day = (int(x) for x in data.split('-'))
    formattedDateTime = datetime.date(year, month, day)
    formattedDay = formattedDateTime.strftime("%A")
    formattedMonth = formattedDateTime.strftime("%B")
    return formattedDay, formattedMonth

def predict_for_created_issues(file, LOCAL_IMAGE_PATH, CREATED_ISSUES_IMAGE_NAME):
    df = pd.read_csv(file)

    # Aggregate the data by date
    df_created = df['created_at'].value_counts().rename_axis('ds').reset_index(name='y')

    # Create a time series model
    model = sm.tsa.ARIMA(df_created['y'].iloc[1:], order=(1, 0, 0))
    results = model.fit()

    # Make predictions
    df_created['forecast'] = results.fittedvalues
    df_created.set_index('ds', inplace=True)

    # Plot the results
    plt.switch_backend("Agg")
    fig, axs = plt.subplots(1, 1, figsize=(10, 4))
    df_created[['y', 'forecast']].plot(ax=axs)
    axs.legend()
    axs.set_title('Created Issues Forecast (Statsmodels)')
    axs.set_xlabel('Date')
    axs.set_ylabel('Created Issues')
    axs.grid(True, linestyle='--', alpha=0.7)

    # Save the figure
    plt.savefig(LOCAL_IMAGE_PATH + CREATED_ISSUES_IMAGE_NAME)
    plt.close()


def predict_for_closed_issues(file, LOCAL_IMAGE_PATH, CLOSED_ISSUES_IMAGE_NAME):
    df = pd.read_csv(file)

    # Aggregate the data by date
    df_created = df['closed_at'].value_counts().rename_axis('ds').reset_index(name='y')

    # Create a time series model
    model = sm.tsa.ARIMA(df_created['y'].iloc[1:], order=(1, 0, 0))
    results = model.fit()

    # Make predictions
    df_created['forecast'] = results.fittedvalues
    df_created.set_index('ds', inplace=True)

    # Plot the results
    plt.switch_backend("Agg")
    fig, axs = plt.subplots(1, 1, figsize=(10, 4))
    df_created[['y', 'forecast']].plot(ax=axs)
    axs.legend()
    axs.set_title('Created Issues Forecast (Statsmodels)')
    axs.set_xlabel('Date')
    axs.set_ylabel('Created Issues')
    axs.grid(True, linestyle='--', alpha=0.7)

    # Save the figure
    plt.savefig(LOCAL_IMAGE_PATH + CLOSED_ISSUES_IMAGE_NAME)
    plt.close()


def predict_for_pulls(filePath, LOCAL_IMAGE_PATH, PULLS_FORECAST_IMAGE_NAME):
    # Read data from the file
    list_of_pulls = [json.loads(line) for line in open(filePath)]
    df_list_pull = pd.DataFrame(list_of_pulls)
    
    # Aggregate the data by date
    pullsDataFrame = df_list_pull.groupby(['created_at']).size().to_frame('y').reset_index()
    pullsDataFrame.rename(columns={'created_at': 'ds'}, inplace=True)

    # Create a time series model
    model = sm.tsa.ARIMA(pullsDataFrame['y'].iloc[1:], order=(1, 0, 1))
    results = model.fit()
    
    # Make predictions
    pullsDataFrame['forecast'] = results.fittedvalues
    
    # Plot the results
    plt.switch_backend("Agg")
    fig, axs = plt.subplots(1, 1, figsize=(15, 8))
    
    # Plot the forecast
    axs.plot(pullsDataFrame['ds'], pullsDataFrame['forecast'], c='red', label='Forecast')

    # Plot the actual data
    axs.plot(pullsDataFrame['ds'], pullsDataFrame['y'], marker='.', label='Data')

    locator = mdates.AutoDateLocator()
    axs.xaxis.set_major_locator(locator)
    axs.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator))
    axs.legend()
    axs.set_title('Actual vs Statsmodel Predicted Graph')
    axs.set_xlabel('Date')
    axs.set_ylabel('Pulls')

    # Save the figure
    plt.savefig(LOCAL_IMAGE_PATH + PULLS_FORECAST_IMAGE_NAME)
    plt.show()


def predict_for_commits(file, LOCAL_IMAGE_PATH, COMMITS_FORECAST_IMAGE_NAME):
    # Read data from the file
    dataFrame = pd.read_csv(file)

    # Aggregate the data by date
    commitsDataFrame = dataFrame.groupby(['ds']).size().to_frame('y').reset_index()

    # Create a time series model
    model = sm.tsa.ARIMA(commitsDataFrame['y'].iloc[1:], order=(1, 0, 1))
    results = model.fit()

    # Make predictions
    commitsDataFrame['forecast'] = results.fittedvalues

    # Plot the results
    plt.switch_backend("Agg")
    fig, axs = plt.subplots(1, 1, figsize=(10, 4))

    # Plot the forecast
    axs.plot(commitsDataFrame['ds'], commitsDataFrame['forecast'], c='red', label='Forecast')

    # Plot the actual data
    axs.plot(commitsDataFrame['ds'], commitsDataFrame['y'], marker='.', label='Data')

    locator = mdates.AutoDateLocator()
    axs.xaxis.set_major_locator(locator)
    axs.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator))
    axs.legend()
    axs.set_title('Commits Forecast (Statsmodels)')
    axs.set_xlabel('Date')
    axs.set_ylabel('Commits')
    axs.grid(True, linestyle='--', alpha=0.7)

    # Save the figure
    plt.savefig(LOCAL_IMAGE_PATH + COMMITS_FORECAST_IMAGE_NAME)
    plt.show()


def predict_for_branches(file, LOCAL_IMAGE_PATH, BRANCHES_FORECAST_IMAGE_NAME):
    # Read data from the file
    dataFrame = pd.read_csv(file)

    # Aggregate the data by date
    branchesDataFrame = dataFrame.groupby(['ds']).size().to_frame('y').reset_index()

    # Create a time series model
    model = sm.tsa.ARIMA(branchesDataFrame['y'].iloc[1:], order=(1, 0, 1))
    results = model.fit()

    # Make predictions
    branchesDataFrame['forecast'] = results.fittedvalues

    # Plot the results
    plt.switch_backend("Agg")
    fig, axs = plt.subplots(1, 1, figsize=(10, 4))

    # Plot the forecast with a red solid line
    axs.plot(branchesDataFrame['ds'], branchesDataFrame['forecast'], color='red', linestyle='-', linewidth=2, label='Forecast')

    # Plot the actual data with a blue marker
    axs.plot(branchesDataFrame['ds'], branchesDataFrame['y'], marker='o', markersize=6, linestyle='', color='blue', label='Actual')

    locator = mdates.AutoDateLocator()
    axs.xaxis.set_major_locator(locator)
    axs.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator))
    axs.legend()
    axs.set_title('Branches Forecast (Statsmodels)')
    axs.set_xlabel('Date')
    axs.set_ylabel('Branches')

    # Save the figure
    plt.savefig(LOCAL_IMAGE_PATH + BRANCHES_FORECAST_IMAGE_NAME)
    plt.show()



def predict_for_releases(file, LOCAL_IMAGE_PATH, RELEASES_FORECAST_IMAGE_NAME):
    # Read data from the file
    dataFrame = pd.read_csv(file)

    # Aggregate the data by date
    releasesDataFrame = dataFrame.groupby(['ds']).size().to_frame('y').reset_index()

    # Create a time series model
    model = sm.tsa.ARIMA(releasesDataFrame['y'].iloc[1:], order=(1, 0, 1))
    results = model.fit()

    # Make predictions
    releasesDataFrame['forecast'] = results.fittedvalues

    # Plot the results
    plt.switch_backend("Agg")
    fig, axs = plt.subplots(1, 1, figsize=(10, 4))

    # Plot the forecast with a red solid line
    axs.plot(releasesDataFrame['ds'], releasesDataFrame['forecast'], color='red', linestyle='-', linewidth=2, label='Forecast')

    # Plot the actual data with a blue marker
    axs.plot(releasesDataFrame['ds'], releasesDataFrame['y'], marker='o', markersize=6, linestyle='', color='blue', label='Actual')

    locator = mdates.AutoDateLocator()
    axs.xaxis.set_major_locator(locator)
    axs.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator))
    axs.legend()
    axs.set_title('Releases Forecast (Statsmodels)')
    axs.set_xlabel('Date')
    axs.set_ylabel('Releases')

    # Save the figure
    plt.savefig(LOCAL_IMAGE_PATH + RELEASES_FORECAST_IMAGE_NAME)
    plt.show()


def predict_for_contributers(file, LOCAL_IMAGE_PATH, CONTRIBUTERS_FORECAST_IMAGE_NAME):
    # Read data from the file
    dataFrame = pd.read_csv(file)

    # Aggregate the data by date
    contributersDataFrame = dataFrame.groupby(['ds']).size().to_frame('y').reset_index()

    # Create a time series model
    model = sm.tsa.ARIMA(contributersDataFrame['y'].iloc[1:], order=(1, 0, 1))
    results = model.fit()

    # Make predictions
    contributersDataFrame['forecast'] = results.fittedvalues

    # Plot the results
    plt.switch_backend("Agg")
    fig, axs = plt.subplots(1, 1, figsize=(10, 4))

    # Plot the forecast with a red solid line
    axs.plot(contributersDataFrame['ds'], contributersDataFrame['forecast'], color='red', linestyle='-', linewidth=2, label='Forecast')

    # Plot the actual data with a blue marker
    axs.plot(contributersDataFrame['ds'], contributersDataFrame['y'], marker='o', markersize=6, linestyle='', color='blue', label='Actual')

    locator = mdates.AutoDateLocator()
    axs.xaxis.set_major_locator(locator)
    axs.xaxis.set_major_formatter(mdates.AutoDateFormatter(locator))
    axs.legend()
    axs.set_title('Contributors Forecast (Statsmodels)')
    axs.set_xlabel('Date')
    axs.set_ylabel('Contributors')

    # Save the figure
    plt.savefig(LOCAL_IMAGE_PATH + CONTRIBUTERS_FORECAST_IMAGE_NAME)
    plt.show()



@app.route('/api/forecast/statsmodel', methods=['POST'])
def forecast():
    body = request.get_json()
    repo_name = body["repo_name"]
    repo_key = body["repo_key"]
    start_date = body["start_date"]
    end_date = body["end_date"]


    # Call the method
    file_name = build_json_file(repo_name, repo_key, start_date, end_date)  

    issues = [json.loads(line) for line in open(file_name)]
    df_new = DataFrame(issues)
    

    # Creating the image path for model loss, LSTM generated image and all issues data image
    CREATED_ISSUES_IMAGE_NAME = "Created_Issues" +"_"+ repo_name + ".png"
    CREATED_ISSUES_URL = BASE_IMAGE_PATH + CREATED_ISSUES_IMAGE_NAME

    # Creating the image path for model loss, LSTM generated image and all issues data image
    CLOSED_ISSUES_IMAGE_NAME = "Closed_Issues" +"_"+ repo_name + ".png"
    CLOSED_ISSUES_URL = BASE_IMAGE_PATH + CLOSED_ISSUES_IMAGE_NAME

    # Creating the image path for model loss, LSTM generated image and all issues data image
    PULLS_FORECAST_IMAGE_NAME = "Pulls_Forecast" +"_"+ repo_name + ".png"
    PULLS_FORECAST_IMAGE_URL = BASE_IMAGE_PATH + PULLS_FORECAST_IMAGE_NAME

    # Creating the image path for model loss, LSTM generated image and all issues data image
    COMMITS_FORECAST_IMAGE_NAME = "Commits_Forecast" +"_"+ repo_name + ".png"
    COMMITS_FORECAST_IMAGE_URL = BASE_IMAGE_PATH + COMMITS_FORECAST_IMAGE_NAME

    # Creating the image path for model loss, LSTM generated image and all issues data image
    BRANCHES_FORECAST_IMAGE_NAME = "Branches_Forecast" +"_"+ repo_name + ".png"
    BRANCHES_FORECAST_IMAGE_URL = BASE_IMAGE_PATH + BRANCHES_FORECAST_IMAGE_NAME

     # Creating the image path for model loss, LSTM generated image and all issues data image
    RELEASES_FORECAST_IMAGE_NAME = "Releases_Forecast" +"_"+ repo_name + ".png"
    RELEASES_FORECAST_IMAGE_URL = BASE_IMAGE_PATH + RELEASES_FORECAST_IMAGE_NAME

     # Creating the image path for model loss, LSTM generated image and all issues data image
    CONTRIBUTERS_FORECAST_IMAGE_NAME = "Contributers_Forecast" +"_"+ repo_name + ".png"
    CONTRIBUTERS_FORECAST_IMAGE_URL = BASE_IMAGE_PATH + CONTRIBUTERS_FORECAST_IMAGE_NAME

    REPO_FILENAME=  BASE_IMAGE_PATH + LOCAL_IMAGE_PATH + repo_name + '.csv'
    
    #Creating csv file
    df_new.to_csv(REPO_FILENAME, sep=',', encoding='utf-8', index=False)

    dateOfWeekMaxCreated = maxIssuesCreatedDate(REPO_FILENAME)
    dateOfWeekMaxClosed = maxIssuesClosedDate(REPO_FILENAME)
    
    dayOfWeekMaxCreated, monthOfYearMaxCreated = convertedDate(dateOfWeekMaxCreated)
    dayOfWeekMaxClosed, monthOfYearMaxClosed = convertedDate(dateOfWeekMaxClosed)


    repos_df = fetchCommitsBranchesReleasesData(repo_name, repo_key)
    commitsfile = getcommitscsv(repos_df,repo_name)
    branchesfile = getbranchescsv(repos_df, repo_name)
    releasesfile =getreleasescv(repos_df, repo_name)
    contributersfile = getcontributerscsv(repos_df, repo_name)

    predict_for_created_issues(REPO_FILENAME, LOCAL_IMAGE_PATH, CREATED_ISSUES_IMAGE_NAME)
    predict_for_closed_issues(REPO_FILENAME, LOCAL_IMAGE_PATH, CLOSED_ISSUES_IMAGE_NAME)
    predict_for_pulls(file_name, LOCAL_IMAGE_PATH, PULLS_FORECAST_IMAGE_NAME)
    predict_for_commits(commitsfile,LOCAL_IMAGE_PATH, COMMITS_FORECAST_IMAGE_NAME)
    predict_for_branches(branchesfile,LOCAL_IMAGE_PATH,BRANCHES_FORECAST_IMAGE_NAME)
    predict_for_releases(releasesfile,LOCAL_IMAGE_PATH,RELEASES_FORECAST_IMAGE_NAME)
    predict_for_contributers(contributersfile,LOCAL_IMAGE_PATH,CONTRIBUTERS_FORECAST_IMAGE_NAME)

    # Uploads an images into the google cloud storage bucket
    new_blob = bucket.blob(CREATED_ISSUES_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + CREATED_ISSUES_IMAGE_NAME)
    new_blob = bucket.blob(CLOSED_ISSUES_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + PULLS_FORECAST_IMAGE_NAME)
    new_blob = bucket.blob(PULLS_FORECAST_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + PULLS_FORECAST_IMAGE_NAME)
    new_blob = bucket.blob(COMMITS_FORECAST_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + COMMITS_FORECAST_IMAGE_NAME)
    new_blob = bucket.blob(BRANCHES_FORECAST_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + BRANCHES_FORECAST_IMAGE_NAME)
    new_blob = bucket.blob(RELEASES_FORECAST_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + RELEASES_FORECAST_IMAGE_NAME)
    new_blob = bucket.blob(CONTRIBUTERS_FORECAST_IMAGE_NAME)
    new_blob.upload_from_filename(
        filename=LOCAL_IMAGE_PATH + CONTRIBUTERS_FORECAST_IMAGE_NAME)
    
    
    json_response = {
    "dayOfWeekMaxCreated": dayOfWeekMaxCreated + ", " + dateOfWeekMaxCreated,
    "dayOfWeekMaxClosed": dayOfWeekMaxClosed + ", " + dateOfWeekMaxClosed,
    "monthOfYearMaxClosed": monthOfYearMaxClosed,
    "createdIssuesForecast" : CREATED_ISSUES_URL,
    "closedIssuesForecast": CLOSED_ISSUES_URL,
    "pullsForecast": PULLS_FORECAST_IMAGE_URL,
    "commitsForecast": COMMITS_FORECAST_IMAGE_URL,
    "branchesForecast": BRANCHES_FORECAST_IMAGE_URL,
    "releasesForecast": RELEASES_FORECAST_IMAGE_URL,
    "contributersForecast": CONTRIBUTERS_FORECAST_IMAGE_URL
    }

    # Returns image url back to flask microservice
    return jsonify(json_response)

# Run Prophet app server on port 8080
if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=9000)
